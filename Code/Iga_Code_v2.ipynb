{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6497bd",
   "metadata": {},
   "source": [
    "# Iterated Prisoner's Dilemma On A Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f664e0b",
   "metadata": {},
   "source": [
    "## Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "857538bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from matplotlib.animation import FuncAnimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "c580c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "plt.rcParams[\"animation.embed_limit\"] = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "2f90464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7e923",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "799dcd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "payoff_matrices = {  # some of these don't look right\n",
    "    \"Default\": {\n",
    "        (\"C\", \"C\"): (3, 3),\n",
    "        (\"C\", \"D\"): (0, 5),\n",
    "        (\"D\", \"C\"): (5, 0),\n",
    "        (\"D\", \"D\"): (0, 0),\n",
    "    },\n",
    "    # \"Canonical\": {\n",
    "    #     (\"C\", \"C\"): (-1, -1),\n",
    "    #     (\"C\", \"D\"): (-3, 0),\n",
    "    #     (\"D\", \"C\"): (0, -3),\n",
    "    #     (\"D\", \"D\"): (-2, -2),\n",
    "    # },\n",
    "    # \"Friend or Foe\": {\n",
    "    #     (\"C\", \"C\"): (1, 1),\n",
    "    #     (\"C\", \"D\"): (0, 2),\n",
    "    #     (\"D\", \"C\"): (2, 0),\n",
    "    #     (\"D\", \"D\"): (0, 0),\n",
    "    # },\n",
    "    # \"Snowdrift\": {\n",
    "    #     (\"C\", \"C\"): (500, 500),\n",
    "    #     (\"C\", \"D\"): (200, 800),\n",
    "    #     (\"D\", \"C\"): (800, 200),\n",
    "    #     (\"D\", \"D\"): (0, 0),\n",
    "    # },\n",
    "    # \"Prisoners\": {\n",
    "    #     (\"C\", \"C\"): (500, 500),\n",
    "    #     (\"C\", \"D\"): (-200, 1200),\n",
    "    #     (\"D\", \"C\"): (1200, -200),\n",
    "    #     (\"D\", \"D\"): (0, 0),\n",
    "    # },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684ac16",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7020407",
   "metadata": {},
   "source": [
    "### Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "a246382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionStrategy:\n",
    "    \"\"\"Strategy that always plays its current action, randomly initialized.\"\"\"\n",
    "\n",
    "    def __init__(self, rng):\n",
    "        self.rng = rng\n",
    "        self.action = \"C\" if self.rng.random() < 0.5 else \"D\"\n",
    "\n",
    "    def decide(self, agent_history):\n",
    "        return self.action\n",
    "\n",
    "    def set_action(self, action):\n",
    "        self.action = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "85ead775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationStrategy(ActionStrategy):\n",
    "    \"\"\"Imitate the action with the highest mean payoff in interactions.\"\"\"\n",
    "\n",
    "    def decide(self, agent_history):\n",
    "        totals = {\"C\": 0.0, \"D\": 0.0}\n",
    "        counts = {\"C\": 0, \"D\": 0}\n",
    "        for interactions in agent_history.values():\n",
    "            for inter in interactions:\n",
    "                counts[inter.own_action] += 1\n",
    "                counts[inter.neighbor_action] += 1\n",
    "                totals[inter.own_action] += inter.own_reward\n",
    "                totals[inter.neighbor_action] += inter.neighbor_reward\n",
    "\n",
    "        mean_C = totals[\"C\"] / counts[\"C\"] if counts[\"C\"] else 0\n",
    "        mean_D = totals[\"D\"] / counts[\"D\"] if counts[\"D\"] else 0\n",
    "\n",
    "        if mean_C > mean_D:\n",
    "            self.action = \"C\"\n",
    "        elif mean_D > mean_C:\n",
    "            self.action = \"D\"\n",
    "        else:  # in case of a tie, continue with the existing strategy\n",
    "            pass\n",
    "\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "11dbd013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FermiStrategy(ActionStrategy):\n",
    "    \"\"\"\n",
    "    Endogenous Fermi algorithm (pairwise).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rng, temperature=0.1):\n",
    "        super().__init__(rng)\n",
    "        self.K = temperature\n",
    "\n",
    "    def decide(self, agent_history):\n",
    "        # if no neighbours yet, keep current action\n",
    "        if not agent_history:\n",
    "            return self.action\n",
    "\n",
    "        # pick one random neighbour we've interacted with\n",
    "        neighbour_id = self.rng.choice(list(agent_history.keys()))\n",
    "        interactions = agent_history.get(neighbour_id, [])\n",
    "        if not interactions:\n",
    "            return self.action\n",
    "\n",
    "        # estimate payoffs from history with that neighbour\n",
    "        own_rewards = [i.own_reward for i in interactions]\n",
    "        neigh_rewards = [i.neighbor_reward for i in interactions]\n",
    "\n",
    "        payoff_self = float(np.mean(own_rewards)) if own_rewards else 0.0\n",
    "        payoff_neigh = float(np.mean(neigh_rewards)) if neigh_rewards else 0.0\n",
    "\n",
    "        delta = payoff_neigh - payoff_self\n",
    "\n",
    "        # Fermi probability\n",
    "        if self.K == 0:\n",
    "            p_switch = 1.0 if delta > 0 else 0.0\n",
    "        else:\n",
    "            exponent = -delta / self.K\n",
    "            exponent = max(min(exponent, 700), -700)  # avoid overflow\n",
    "            p_switch = 1.0 / (1.0 + math.exp(exponent))\n",
    "\n",
    "        # switch action to neighbour's most recently observed action\n",
    "        if self.rng.random() < p_switch:\n",
    "            self.action = interactions[-1].neighbor_action\n",
    "\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "d37d59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcementLearningStrategy(ActionStrategy):\n",
    "    \"\"\"\n",
    "    Q-learning strategy with epsilon-greedy action selection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rng, learning_rate=0.1, epsilon=0.1, initial_q=0.0):\n",
    "        super().__init__(rng)\n",
    "        self.alpha = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.q = {\"C\": float(initial_q), \"D\": float(initial_q)}\n",
    "        self._last_action = None\n",
    "        self._last_reward = 0.0\n",
    "\n",
    "    def decide(self, agent_history):\n",
    "        # observe most recent reward from any interaction (if exists)\n",
    "        last = None\n",
    "        for interactions in agent_history.values():\n",
    "            if interactions:\n",
    "                cand = interactions[-1]\n",
    "                if last is None:\n",
    "                    last = cand\n",
    "        if last is not None:\n",
    "            self._last_reward = last.own_reward\n",
    "\n",
    "        # update Q for the action we previously played\n",
    "        if self._last_action is not None:\n",
    "            a = self._last_action\n",
    "            self.q[a] = self.q[a] + self.alpha * (self._last_reward - self.q[a])\n",
    "\n",
    "        # choose next action (epsilon-greedy)\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            action = \"C\" if self.rng.random() < 0.5 else \"D\"\n",
    "        else:\n",
    "            if self.q[\"C\"] > self.q[\"D\"]:\n",
    "                action = \"C\"\n",
    "            elif self.q[\"D\"] > self.q[\"C\"]:\n",
    "                action = \"D\"\n",
    "            else:\n",
    "                action = \"C\" if self.rng.random() < 0.5 else \"D\"\n",
    "\n",
    "        self.action = action\n",
    "        self._last_action = action\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e4e0a",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "dc03a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Minimal agent holding a strategy, payoff, and history.\"\"\"\n",
    "\n",
    "    @dataclass\n",
    "    class Interaction:\n",
    "        own_action: str\n",
    "        own_reward: float\n",
    "        neighbor_action: str\n",
    "        neighbor_reward: float\n",
    "\n",
    "    def __init__(self, agent_id, strategy, history_window=5, store_history=True):\n",
    "        self.id = agent_id\n",
    "        self.strategy = strategy\n",
    "        self.history = {}\n",
    "        self.payoff = 0.0\n",
    "        self.history_window = history_window\n",
    "        self.store_history = store_history\n",
    "\n",
    "    def choose_action(self):\n",
    "        return self.strategy.decide(self.history)\n",
    "\n",
    "    def record_interaction(\n",
    "        self, neighbor_id, own_action, neighbor_action, own_reward, neighbor_reward\n",
    "    ):\n",
    "        self.payoff += own_reward\n",
    "        if not self.store_history:\n",
    "            return\n",
    "        lst = self.history.setdefault(neighbor_id, [])\n",
    "        lst.append(\n",
    "            self.Interaction(own_action, own_reward, neighbor_action, neighbor_reward)\n",
    "        )\n",
    "        if len(lst) > self.history_window:\n",
    "            del lst[: -self.history_window]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "40443196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Jaccard Index for stability\n",
    "def jaccard_indices(set1, set2):\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# Cluster sizes\n",
    "def cluster_sizes(graph, cooperators):\n",
    "    subgraph = graph.subgraph(cooperators)\n",
    "    return [len(c) for c in nx.connected_components(subgraph)]\n",
    "\n",
    "# Largest cooperative fraction\n",
    "def largest_cooperative_fraction(graph, cooperators):\n",
    "    subgraph = graph.subgraph(cooperators)\n",
    "    largest_cluster = max(nx.connected_components(subgraph), key=len, default=[])\n",
    "    return len(largest_cluster) / graph.number_of_nodes()\n",
    "\n",
    "# Stability of largest cooperative cluster\n",
    "def stability(largest_cluster_prev, largest_cluster_curr):\n",
    "    if not largest_cluster_prev or not largest_cluster_curr:\n",
    "        return 0.0\n",
    "    return len(largest_cluster_prev & largest_cluster_curr) / len(largest_cluster_curr)\n",
    "\n",
    "\n",
    "# Average time nodes spend in the largest cluster\n",
    "def node_times_in_cluster(node_times, current_cluster):\n",
    "    for node in current_cluster:\n",
    "        node_times[node] = node_times.get(node, 0) + 1\n",
    "    return sum(node_times.values()) / len(node_times) if node_times else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28347de",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "5a215c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x, window=20):\n",
    "    x = np.asarray(x)\n",
    "    if len(x) < window:\n",
    "        return x\n",
    "    return np.convolve(x, np.ones(window)/window, mode=\"valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "d5a1747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sets(A, B):\n",
    "    if not A and not B:\n",
    "        return 1.0\n",
    "    if not A or not B:\n",
    "        return 0.0\n",
    "    return len(A & B) / len(A | B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "0817c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_length(graph, cooperators):\n",
    "    count = 0\n",
    "    for u, v in graph.edges():\n",
    "        if (u in cooperators) != (v in cooperators):\n",
    "            count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "f43f3c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(fig, filename, output_dir=\"plots\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    fig.savefig(filepath)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "e6e734d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_cluster_sizes(results, Tburn, strategy_label):\n",
    "    sizes = []\n",
    "    for r in results:\n",
    "        for step_sizes in r[strategy_label][Tburn:]:\n",
    "            sizes.extend(step_sizes)\n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "6b780e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_runs(results, Tburn=0):\n",
    "    keys = [\"Smax\", \"c\", \"JV\", \"JL\", \"Neff\", \"B\"]\n",
    "    L = min(len(r[k]) for r in results for k in keys)\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        X = np.stack([r[k][:L] for r in results])\n",
    "        out[k] = {\n",
    "            \"mean_ts\": X.mean(axis=0),\n",
    "            \"std_ts\": X.std(axis=0, ddof=1),\n",
    "            \"mean_scalar\": X[:, Tburn:].mean(),\n",
    "        }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "4563bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_cluster_sizes_window(results, t_start, t_end):\n",
    "    sizes = []\n",
    "    for r in results:\n",
    "        for step_sizes in r[\"sizes\"][t_start:t_end]:\n",
    "            sizes.extend(step_sizes)\n",
    "    return sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "c9c24931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_Smax_vs_c(results, Tburn):\n",
    "    xs, ys = [], []\n",
    "    for r in results:\n",
    "        c = r[\"c\"][Tburn:]\n",
    "        Smax = r[\"Smax\"][Tburn:]\n",
    "        xs.extend(c)\n",
    "        ys.extend(Smax)\n",
    "    return np.array(xs), np.array(ys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fb92e",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "3f5f318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"NetworkX graph wrapper.\"\"\"\n",
    "\n",
    "    def generate_graph(self, kind, n, seed=None, **kwargs):\n",
    "        \"\"\"Generate a networkx graph by name.\"\"\"\n",
    "        if kind == \"grid\":\n",
    "            side_length = int(math.sqrt(n))\n",
    "            graph = nx.convert_node_labels_to_integers(\n",
    "                nx.grid_2d_graph(side_length, side_length)\n",
    "            )\n",
    "\n",
    "        elif kind == \"stochastic_block\":\n",
    "            sizes = kwargs.pop(\"sizes\")\n",
    "            p = kwargs.pop(\"p\")\n",
    "            graph = nx.stochastic_block_model(sizes, p, seed=seed, **kwargs)\n",
    "\n",
    "        else:\n",
    "            generators = {\n",
    "                \"erdos_renyi\": nx.erdos_renyi_graph,\n",
    "                \"watts_strogatz\": nx.watts_strogatz_graph,\n",
    "                \"barabasi_albert\": nx.barabasi_albert_graph,\n",
    "            }\n",
    "            graph = generators[kind](n, seed=seed, **kwargs)\n",
    "\n",
    "        self.kind = kind\n",
    "        self.graph = graph\n",
    "        self.seed = seed\n",
    "\n",
    "    def neighbour(self, agent_id):\n",
    "        \"\"\"Return neighbour agent IDs.\"\"\"\n",
    "        if self.graph:\n",
    "            return list(self.graph.neighbors(agent_id))\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "359370c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSimulation(Network):\n",
    "    \"\"\"\n",
    "    Base class for running evolutionary games on any NetworkX graph.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        kind=\"grid\",\n",
    "        n=400,\n",
    "        seed=42,\n",
    "        rounds=100,\n",
    "        strategy=ActionStrategy,\n",
    "        strategy_kwargs=None,\n",
    "        payoff_matrix=payoff_matrices[\"Default\"],\n",
    "        rng=None,\n",
    "        history_window=5,\n",
    "        store_history=True,\n",
    "        store_snapshots=True,\n",
    "        **graph_kwargs,\n",
    "    ):\n",
    "        self.strategy = strategy\n",
    "        self.strategy_kwargs = strategy_kwargs or {}\n",
    "        self.rounds = rounds\n",
    "        self.payoff_matrix = payoff_matrix\n",
    "        self.rng = rng if rng is not None else np.random.default_rng(seed)\n",
    "        self.history_window = history_window\n",
    "        self.store_history = store_history\n",
    "        self.store_snapshots = store_snapshots\n",
    "        self.generate_graph(kind=kind, n=n, seed=seed, **graph_kwargs)\n",
    "        print(nx.degree_histogram(self.graph))\n",
    "        print(nx.average_clustering(self.graph))\n",
    "        self.edge_list = list(self.graph.edges())\n",
    "        self.agents = {}\n",
    "        self.snapshots = []\n",
    "        self._initialize_agents()\n",
    "        self.jaccard_indices = []\n",
    "        self.cluster_sizes = []\n",
    "        self.num_components = []\n",
    "        self.set_stability = []\n",
    "\n",
    "        self.largest_cooperative_fraction = []\n",
    "        self.stability = []\n",
    "        self.cooperation_fraction = []\n",
    "        self.node_times_in_cluster = {}\n",
    "        self.prev_largest_cluster = set()\n",
    "        self.prev_cooperators = set()\n",
    "        self.effective_num_components = []\n",
    "        self.boundary_per_node = []\n",
    "        self.defector_cluster_sizes = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _initialize_agents(self):\n",
    "        for agent_id in self.graph.nodes:\n",
    "            strat = self.strategy(self.rng, **self.strategy_kwargs)\n",
    "            self.agents[agent_id] = Agent(\n",
    "                agent_id,\n",
    "                strat,\n",
    "                history_window=self.history_window,\n",
    "                store_history=self.store_history,\n",
    "            )\n",
    "\n",
    "    def _reset_payoffs(self):\n",
    "        for agent in self.agents.values():\n",
    "            agent.payoff = 0.0\n",
    "\n",
    "    # fast inner loop: precompute lookups and actions once\n",
    "    def _play_round(self):\n",
    "        agents = self.agents\n",
    "        payoff_matrix = self.payoff_matrix\n",
    "        edge_list = self.edge_list\n",
    "\n",
    "        for agent in agents.values():\n",
    "            agent.payoff = 0.0\n",
    "\n",
    "        actions = {node: agents[node].choose_action() for node in agents}\n",
    "        for node_a, node_b in edge_list:\n",
    "            action_a = actions[node_a]\n",
    "            action_b = actions[node_b]\n",
    "            payoff_a, payoff_b = payoff_matrix.get((action_a, action_b), (0, 0))\n",
    "            agents[node_a].record_interaction(\n",
    "                node_b, action_a, action_b, payoff_a, payoff_b\n",
    "            )\n",
    "            agents[node_b].record_interaction(\n",
    "                node_a, action_b, action_a, payoff_b, payoff_a\n",
    "            )\n",
    "    \n",
    "    def _update_cluster_metrics(self):\n",
    "        cooperators = {\n",
    "            node for node, agent in self.agents.items()\n",
    "            if agent.strategy.action == \"C\"\n",
    "        }\n",
    "        \n",
    "        c_frac = len(cooperators) / self.graph.number_of_nodes()\n",
    "        self.cooperation_fraction.append(c_frac)\n",
    "        sizes = cluster_sizes(self.graph, cooperators)\n",
    "        self.cluster_sizes.append(sizes)\n",
    "        subgraph = self.graph.subgraph(cooperators)\n",
    "        largest_cluster = set(\n",
    "            max(nx.connected_components(subgraph), key=len, default=[])\n",
    "        )\n",
    "        num_components = len(sizes)\n",
    "        self.num_components.append(num_components)\n",
    "        if self.prev_largest_cluster:\n",
    "            jac = jaccard_indices(self.prev_largest_cluster, largest_cluster)\n",
    "        else:\n",
    "            jac = 0.0\n",
    "        self.jaccard_indices.append(jac)\n",
    "        stab = stability(self.prev_largest_cluster, largest_cluster)\n",
    "        self.stability.append(stab)\n",
    "        frac = len(largest_cluster) / self.graph.number_of_nodes()\n",
    "        self.largest_cooperative_fraction.append(frac)\n",
    "        avg_time = node_times_in_cluster(\n",
    "            self.node_times_in_cluster, largest_cluster\n",
    "        )\n",
    "        if hasattr(self, \"prev_cooperators\"):\n",
    "            JV = jaccard_sets(self.prev_cooperators, cooperators)\n",
    "        else:\n",
    "            JV = 0.0\n",
    "        self.set_stability.append(JV)\n",
    "        self.prev_cooperators = cooperators\n",
    "        if sizes:\n",
    "            total = sum(sizes)\n",
    "            Neff = (total ** 2) / sum(s * s for s in sizes)\n",
    "        else:\n",
    "            Neff = 0.0\n",
    "\n",
    "        self.effective_num_components.append(Neff)\n",
    "        B = boundary_length(self.graph, cooperators)\n",
    "        if cooperators:\n",
    "            B_norm = B / len(cooperators)\n",
    "        else:\n",
    "            B_norm = 0.0\n",
    "\n",
    "        self.boundary_per_node.append(B_norm)\n",
    "        defectors = set(self.graph.nodes()) - cooperators\n",
    "        defector_sizes = cluster_sizes(self.graph, defectors)\n",
    "        self.defector_cluster_sizes.append(defector_sizes)\n",
    "        self.prev_largest_cluster = largest_cluster\n",
    "\n",
    "\n",
    "    def _get_state(self):\n",
    "        return {\n",
    "            node_id: (1 if agent.strategy.action == \"D\" else 0)\n",
    "            for node_id, agent in self.agents.items()\n",
    "        }\n",
    "\n",
    "    def encode_state(self):  # for speed\n",
    "        arr = np.fromiter(\n",
    "            (\n",
    "                1 if self.agents[i].strategy.action == \"D\" else 0\n",
    "                for i in self.graph.nodes()\n",
    "            ),\n",
    "            dtype=np.uint8,\n",
    "            count=self.graph.number_of_nodes(),\n",
    "        )\n",
    "        return np.packbits(arr, bitorder=\"little\").tobytes()\n",
    "\n",
    "    def decode_state(self, packed):\n",
    "        n = self.graph.number_of_nodes()\n",
    "        bits = np.unpackbits(np.frombuffer(packed, dtype=np.uint8), bitorder=\"little\")\n",
    "        return bits[:n].astype(np.uint8)\n",
    "\n",
    "    def step(self):\n",
    "        self._play_round()\n",
    "        if self.store_snapshots:\n",
    "            self.snapshots.append(self._get_state())\n",
    "        self._update_cluster_metrics()\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.rounds):\n",
    "            self.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91326bdd",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "ab542719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(\n",
    "    model_class,\n",
    "    strategy_class,\n",
    "    strategy_kwargs=None,\n",
    "    steps=100,\n",
    "    seed=42,\n",
    "    interval=300,\n",
    "    payoff_matrix=payoff_matrices[\"Default\"],\n",
    "    title=\"\",\n",
    "    kind=\"grid\",\n",
    "    n=400,\n",
    "    is_grid=False,\n",
    "    **graph_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Produce animations showing the network state over time.\n",
    "    \"\"\"\n",
    "    payoff_matrix = payoff_matrix\n",
    "    strategy_kwargs = strategy_kwargs or {}\n",
    "    model = model_class(\n",
    "        kind=kind,\n",
    "        n=n,\n",
    "        seed=seed,\n",
    "        rounds=steps,\n",
    "        payoff_matrix=payoff_matrix,\n",
    "        strategy=strategy_class,\n",
    "        strategy_kwargs=strategy_kwargs,\n",
    "        **graph_kwargs,\n",
    "    )\n",
    "\n",
    "    graph = model.graph\n",
    "    n_nodes = graph.number_of_nodes()\n",
    "\n",
    "    C_COOP, C_DEFECT = \"#40B0A6\", \"#FFBE6A\"\n",
    "    cmap = ListedColormap([C_COOP, C_DEFECT])\n",
    "    fig, (ax_sim, ax_stats) = plt.subplots(\n",
    "        2, 1, figsize=(7, 8), gridspec_kw={\"height_ratios\": [4, 1]}\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Stats plot (C% and D%)\n",
    "    # -------------------------\n",
    "    xs, ys_c, ys_d = [], [], []\n",
    "\n",
    "    (line_c,) = ax_stats.plot([], [], lw=2, label=\"C\")\n",
    "    (line_d,) = ax_stats.plot([], [], lw=2, label=\"D\")\n",
    "\n",
    "    ax_stats.set_xlim(0, steps)\n",
    "    ax_stats.set_ylim(0, 100)\n",
    "    ax_stats.yaxis.set_major_formatter(PercentFormatter(xmax=100))\n",
    "    ax_stats.set_ylabel(\"Population\")\n",
    "    ax_stats.grid(True, linestyle=\":\", alpha=0.4)\n",
    "    ax_stats.legend(frameon=False, ncol=2, loc=\"upper right\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Simulation plot\n",
    "    # -------------------------\n",
    "    if is_grid:\n",
    "        dim = int(math.isqrt(n_nodes))\n",
    "        if dim * dim != n_nodes:\n",
    "            raise ValueError(f\"Grid mode needs square number of nodes, got {n_nodes}.\")\n",
    "\n",
    "        def state_as_grid():\n",
    "            state = model._get_state()\n",
    "            grid = [[0] * dim for _ in range(dim)]\n",
    "            for node, val in state.items():\n",
    "                grid[node // dim][node % dim] = val\n",
    "            return grid\n",
    "\n",
    "        sim_artist = ax_sim.imshow(state_as_grid(), cmap=cmap, vmin=0, vmax=1)\n",
    "        ax_sim.set_xticks([])\n",
    "        ax_sim.set_yticks([])\n",
    "\n",
    "        def update_sim():\n",
    "            sim_artist.set_data(state_as_grid())\n",
    "\n",
    "    else:\n",
    "        pos = nx.spring_layout(graph, seed=seed)\n",
    "        nodelist = list(graph.nodes())\n",
    "        nx.draw_networkx_edges(graph, pos, ax=ax_sim, alpha=0.3, edge_color=\"gray\")\n",
    "        state0 = model._get_state()\n",
    "        sim_artist = nx.draw_networkx_nodes(\n",
    "            graph,\n",
    "            pos,\n",
    "            nodelist=nodelist,\n",
    "            node_color=[state0[i] for i in nodelist],\n",
    "            cmap=cmap,\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            node_size=80,\n",
    "            edgecolors=\"gray\",\n",
    "            ax=ax_sim,\n",
    "        )\n",
    "        ax_sim.axis(\"off\")\n",
    "\n",
    "        def update_sim():\n",
    "            state = model._get_state()\n",
    "            sim_artist.set_array([state[i] for i in nodelist])\n",
    "\n",
    "    # -------------------------\n",
    "    # Animation update\n",
    "    # -------------------------\n",
    "    def update(frame):\n",
    "        if frame > 0:\n",
    "            model.step()\n",
    "\n",
    "        ax_sim.set_title(f\"{title} (Step {frame}/{steps})\")\n",
    "\n",
    "        update_sim()\n",
    "\n",
    "        state = model._get_state()\n",
    "        d = sum(state.values())\n",
    "        c = n_nodes - d\n",
    "\n",
    "        xs.append(frame)\n",
    "        ys_c.append(100 * c / n_nodes)\n",
    "        ys_d.append(100 * d / n_nodes)\n",
    "\n",
    "        line_c.set_data(xs, ys_c)\n",
    "        line_d.set_data(xs, ys_d)\n",
    "\n",
    "        return sim_artist, line_c, line_d\n",
    "\n",
    "    \n",
    "    anim= FuncAnimation(\n",
    "        fig,\n",
    "        update,\n",
    "        frames=steps + 1,\n",
    "        interval=interval,\n",
    "        blit=True,\n",
    "        repeat=False,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab42e954",
   "metadata": {},
   "source": [
    "## Helpers for plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "de1214a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = {\n",
    "    # \"ActionStrategy\": ActionStrategy,\n",
    "    \"ImitationStrategy\": ImitationStrategy,\n",
    "    # \"FermiStrategy\": FermiStrategy,\n",
    "    # \"ReinforcementLearningStrategy\": ReinforcementLearningStrategy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "e9183c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(data, key, ylabel, title, ylim=None, filename=None):\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    t = range(len(data[key][\"mean_ts\"]))\n",
    "    ax.plot(t, data[key][\"mean_ts\"], lw=2)\n",
    "    ax.fill_between(\n",
    "        t,\n",
    "        data[key][\"mean_ts\"] - data[key][\"std_ts\"],\n",
    "        data[key][\"mean_ts\"] + data[key][\"std_ts\"],\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    ax.set_xlabel(\"Time step\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    if ylim:\n",
    "        ax.set_ylim(ylim)\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    if filename:\n",
    "        save_plot(fig, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "565a52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data, xlabel, ylabel, title, filename=None):\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.hist(data, bins=30, log=True)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    if filename:\n",
    "        save_plot(fig, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "4f97887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once(seed, **kwargs):\n",
    "    model = NetworkSimulation(seed=seed, **kwargs)\n",
    "    model.run()\n",
    "    return {\n",
    "        \"Smax\": np.array(model.largest_cooperative_fraction),\n",
    "        \"c\":    np.array(model.cooperation_fraction),\n",
    "        \"JV\":   np.array(model.set_stability),\n",
    "        \"JL\":   np.array(model.jaccard_indices),\n",
    "        \"Neff\": np.array(model.effective_num_components),\n",
    "        \"B\":    np.array(model.boundary_per_node),\n",
    "        \"sizes\": model.cluster_sizes,\n",
    "        \"defector_sizes\": model.defector_cluster_sizes,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "ccd63d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_resolved_cluster_distributions(\n",
    "    results,\n",
    "    Tburn,\n",
    "    strategy,\n",
    "    matrix,\n",
    "    filename=None,\n",
    "):\n",
    "    windows = {\n",
    "        \"Early\": (Tburn, Tburn + 30),\n",
    "        \"Mid\":   (Tburn + 50, Tburn + 80),\n",
    "        \"Late\":  (Tburn + 110, Tburn + 150),\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    for label, (t0, t1) in windows.items():\n",
    "        sizes = collect_cluster_sizes_window(results, t0, t1)\n",
    "\n",
    "        ax.hist(\n",
    "            sizes,\n",
    "            bins=30,\n",
    "            density=True,\n",
    "            histtype=\"step\",\n",
    "            log=True,\n",
    "            label=label,\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Cooperative cluster size $s$\")\n",
    "    ax.set_ylabel(\"Probability density (log scale)\")\n",
    "    ax.set_title(\n",
    "        f\"Time-resolved cooperative cluster distributions\\n({strategy}, {matrix})\"\n",
    "    )\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename:\n",
    "        save_plot(fig, filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "9a7d8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Smax_vs_cooperation(\n",
    "    results,\n",
    "    Tburn,\n",
    "    strategy,\n",
    "    matrix,\n",
    "    filename=None,\n",
    "):\n",
    "    c_vals, Smax_vals = collect_Smax_vs_c(results, Tburn)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.scatter(\n",
    "        c_vals,\n",
    "        Smax_vals,\n",
    "        alpha=0.2,\n",
    "        s=10,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Overall cooperation fraction $|V_C|/N$\")\n",
    "    ax.set_ylabel(\"Largest cooperative cluster fraction $S_{\\\\max}$\")\n",
    "    ax.set_title(\n",
    "        f\"Cluster dominance vs cooperation\\n({strategy}, {matrix})\"\n",
    "    )\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename:\n",
    "        save_plot(fig, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "52bf074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation():\n",
    "    seeds = range(20)  \n",
    "    Tburn = 50  \n",
    "\n",
    "    for strategy_name, strategy_class in strategies.items():\n",
    "        for matrix_name, payoff_matrix in payoff_matrices.items():\n",
    "            results = [\n",
    "                run_once(\n",
    "                    seed=s,\n",
    "                    kind=\"grid\",\n",
    "                    n=400,\n",
    "                    rounds=200,\n",
    "                    strategy=strategy_class,\n",
    "                    payoff_matrix=payoff_matrix,\n",
    "                )\n",
    "                for s in seeds\n",
    "            ]\n",
    "\n",
    "            out = aggregate_runs(results, Tburn=Tburn)\n",
    "\n",
    "            plot_time_series(\n",
    "                out,\n",
    "                \"Smax\",\n",
    "                ylabel=\"Largest cooperative component fraction $S_{\\\\max}(t)$\",\n",
    "                title=f\"Evolution of the Largest Cooperative Cluster ({strategy_name}, {matrix_name})\",\n",
    "                ylim=(0, 0.2),\n",
    "                filename=f\"{strategy_name}_{matrix_name}_Smax.png\",\n",
    "            )\n",
    "            plot_time_series(\n",
    "                out,\n",
    "                \"c\",\n",
    "                ylabel=\"Fraction of cooperators $|V_C(t)| / N$\",\n",
    "                title=f\"Overall Level of Cooperation ({strategy_name}, {matrix_name})\",\n",
    "                ylim=(0, 0.6),\n",
    "                filename=f\"{strategy_name}_{matrix_name}_cooperation.png\",\n",
    "            )\n",
    "            plot_time_series(\n",
    "                out,\n",
    "                \"JV\",\n",
    "                ylabel=\"Jaccard index $J_V(t)$\",\n",
    "                title=f\"Stability of the Cooperative Set ({strategy_name}, {matrix_name})\",\n",
    "                ylim=(0, 1),\n",
    "                filename=f\"{strategy_name}_{matrix_name}_JV.png\",\n",
    "            )\n",
    "            plot_time_series(\n",
    "                out,\n",
    "                \"JL\",\n",
    "                ylabel=\"Overlap of largest component $J_L(t)$\",\n",
    "                title=f\"Temporal Stability of the Largest Cooperative Cluster ({strategy_name}, {matrix_name})\",\n",
    "                ylim=(0, 1),\n",
    "                filename=f\"{strategy_name}_{matrix_name}_JL.png\",\n",
    "            )\n",
    "            plot_time_series(\n",
    "                out,\n",
    "                \"Neff\",\n",
    "                ylabel=\"Effective number of cooperative components $N_{\\\\mathrm{eff}}(t)$\",\n",
    "                title=f\"Fragmentation of Cooperative Clusters ({strategy_name}, {matrix_name})\",\n",
    "                filename=f\"{strategy_name}_{matrix_name}_Neff.png\",\n",
    "            )\n",
    "\n",
    "\n",
    "            coop_sizes = collect_cluster_sizes(results, Tburn, \"sizes\")\n",
    "            def_sizes = collect_cluster_sizes(results, Tburn, \"defector_sizes\")\n",
    "\n",
    "            plot_histogram(\n",
    "                coop_sizes,\n",
    "                xlabel=\"Cooperative cluster size $s$\",\n",
    "                ylabel=\"Frequency (log scale)\",\n",
    "                title=f\"Distribution of Cooperative Cluster Sizes ({strategy_name}, {matrix_name})\",\n",
    "                filename=f\"{strategy_name}_{matrix_name}_coop_sizes.png\",\n",
    "            )\n",
    "            plot_histogram(\n",
    "                def_sizes,\n",
    "                xlabel=\"Defector cluster size $s$\",\n",
    "                ylabel=\"Frequency (log scale)\",\n",
    "                title=f\"Distribution of Defector Cluster Sizes ({strategy_name}, {matrix_name})\",\n",
    "                filename=f\"{strategy_name}_{matrix_name}_def_sizes.png\",\n",
    "            )\n",
    "\n",
    "            plot_time_resolved_cluster_distributions(\n",
    "                results=results,\n",
    "                Tburn=Tburn,\n",
    "                strategy=strategy_name,\n",
    "                matrix=matrix_name,\n",
    "                filename=f\"{strategy_name}_{matrix_name}_time_resolved_clusters.png\",\n",
    "\n",
    "            )\n",
    "\n",
    "            plot_Smax_vs_cooperation(\n",
    "                results=results,\n",
    "                Tburn=Tburn,\n",
    "                strategy=strategy_name,\n",
    "                matrix=matrix_name,\n",
    "                filename=f\"{strategy_name}_{matrix_name}_Smax_vs_cooperation.png\",\n",
    "\n",
    "            )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "862bfb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n",
      "[0, 0, 4, 72, 324]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "run_simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
