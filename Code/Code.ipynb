{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6497bd",
   "metadata": {},
   "source": [
    "# Simulating the Iterated Prisoner's Dilemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857538bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import logging\n",
    "from functools import partial\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "plt.rcParams[\"animation.embed_limit\"] = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684ac16",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payoff_matrices = {\n",
    "    \"Default\": {\n",
    "        (\"C\", \"C\"): (3, 3),\n",
    "        (\"C\", \"D\"): (0, 5),\n",
    "        (\"D\", \"C\"): (5, 0),\n",
    "        (\"D\", \"D\"): (0, 0),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3936b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"Store the actions taken in a single interaction.\"\"\"\n",
    "\n",
    "    def __init__(self, own_action, neighbor_action):\n",
    "        self.own_action = own_action\n",
    "        self.neighbor_action = neighbor_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionStrategy():\n",
    "    \"\"\"Strategy that always plays its current action.\"\"\"\n",
    "\n",
    "    def __init__(self, action):\n",
    "        self.action = action\n",
    "\n",
    "    def decide(self, agent_id, own_history, neighbors_history):\n",
    "        return self.action\n",
    "\n",
    "    def set_action(self, action):\n",
    "        self.action = action\n",
    "\n",
    "\n",
    "class RandomActionStrategy(ActionStrategy):\n",
    "    \"\"\"Start with a random action based on a specific cooperation rate.\"\"\"\n",
    "\n",
    "    def __init__(self, rng=None, c_rate=0.5):\n",
    "        rng = rng or random.Random()\n",
    "        # If random float [0.0, 1.0) is less than rate, choose C\n",
    "        action = \"C\" if rng.random() < c_rate else \"D\"\n",
    "        super().__init__(action)\n",
    "\n",
    "\n",
    "class ImitationStrategy(ActionStrategy):\n",
    "      \n",
    "      \"\"\"Strategy that imitates the best performing (highest avg. payoff) strategy among neighoburs.\"\"\"\n",
    "\n",
    "    def decide(self, agent_id, own_history, neighbors_history):\n",
    "        return self.action\n",
    "\n",
    "    def set_action(self, action):\n",
    "        self.action = action  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Minimal agent holding a strategy, payoff, and per-neighbor history.\"\"\"\n",
    "\n",
    "    def __init__(self, agent_id, strategy):\n",
    "        self.id = agent_id\n",
    "        self.strategy = strategy\n",
    "        self.payoff = 0.0\n",
    "        self.history = {}\n",
    "\n",
    "    def choose_action(self, neighbors_history):\n",
    "        \"\"\"Select an action using the configured strategy.\"\"\"\n",
    "        return self.strategy.decide(self.id, self.history, neighbors_history)\n",
    "\n",
    "    def record_interaction(self, neighbor_id, own_action, neighbor_action, reward):\n",
    "        \"\"\"Record interaction outcome and update payoff.\"\"\"\n",
    "        self.history.setdefault(neighbor_id, []).append(\n",
    "            Interaction(own_action, neighbor_action)\n",
    "        )\n",
    "        self.payoff += reward\n",
    "\n",
    "    def neighbors_interactions(self, neighbor_id):\n",
    "        \"\"\"Return the interaction history with a given neighbor.\"\"\"\n",
    "        return list(self.history.get(neighbor_id, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"Network wrapper around a networkx graph.\"\"\"\n",
    "\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "\n",
    "    def neighbors(self, node_id):\n",
    "        \"\"\"Return neighbor node ids.\"\"\"\n",
    "        return self.graph.neighbors(node_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f73875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(kind, n, seed=None, **kwargs):\n",
    "    \"\"\"Generate a networkx graph by name.\"\"\"\n",
    "    if kind == \"grid\":\n",
    "        cols = kwargs.pop(\"m\", n)\n",
    "        graph = nx.grid_2d_graph(n, cols)\n",
    "        return nx.convert_node_labels_to_integers(graph)\n",
    "\n",
    "    generators = {\n",
    "        \"erdos_renyi\": nx.erdos_renyi_graph,\n",
    "        \"watts_strogatz\": nx.watts_strogatz_graph,\n",
    "        \"barabasi_albert\": nx.barabasi_albert_graph,\n",
    "    }\n",
    "    if kind not in generators:\n",
    "        raise ValueError(f\"Unknown graph kind: {kind}\")\n",
    "    return generators[kind](n, seed=seed, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33631f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network(graph):\n",
    "    \"\"\"Wrap a networkx graph to keep the simulator graph-agnostic.\"\"\"\n",
    "    if not isinstance(graph, nx.Graph):\n",
    "        raise TypeError(\"graph must be a networkx Graph\")\n",
    "    return Network(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c06317",
   "metadata": {},
   "source": [
    "## Network simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359370c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSimulation:\n",
    "    \"\"\"\n",
    "    Base class for running evolutionary games on any NetworkX graph.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph,\n",
    "        rounds=50,\n",
    "        seed=None,\n",
    "        payoff_matrix=payoff_matrices[\"Default\"],\n",
    "        initial_coop_rate=0.5,\n",
    "    ):\n",
    "        self.rounds = rounds\n",
    "        self.seed = seed\n",
    "        self.payoff_matrix = payoff_matrix\n",
    "        self.initial_coop_rate = initial_coop_rate  # Store the rate\n",
    "        self.random = random.Random(seed)\n",
    "\n",
    "        self.graph = graph\n",
    "        self.network = Network(self.graph)\n",
    "\n",
    "        self.agents = {}\n",
    "        self.snapshots = []\n",
    "        self._init_agents()\n",
    "\n",
    "    def _init_agents(self):\n",
    "        \"\"\"Create agents with random starting actions for every node in the graph.\"\"\"\n",
    "        for node_id in self.graph.nodes:\n",
    "            # Pass the stored initial_coop_rate to the strategy\n",
    "            strategy = RandomActionStrategy(self.random, c_rate=self.initial_coop_rate)\n",
    "            self.agents[node_id] = Agent(node_id, strategy)\n",
    "\n",
    "    # ... (Rest of the class remains unchanged: _reset_payoffs, _play_round, etc.)\n",
    "    def _reset_payoffs(self):\n",
    "        for agent in self.agents.values():\n",
    "            agent.payoff = 0.0\n",
    "\n",
    "    def _play_round(self):\n",
    "        self._reset_payoffs()\n",
    "        for node_a, node_b in self.graph.edges:\n",
    "            agent_a = self.agents[node_a]\n",
    "            agent_b = self.agents[node_b]\n",
    "\n",
    "            action_a = agent_a.strategy.decide(agent_a.id, agent_a.history, {})\n",
    "            action_b = agent_b.strategy.decide(agent_b.id, agent_b.history, {})\n",
    "\n",
    "            payoff_a, payoff_b = self.payoff_matrix.get((action_a, action_b), (0, 0))\n",
    "\n",
    "            agent_a.record_interaction(node_b, action_a, action_b, payoff_a)\n",
    "            agent_b.record_interaction(node_a, action_b, action_a, payoff_b)\n",
    "\n",
    "    def _update_strategies(self):\n",
    "        raise NotImplementedError(\"Subclasses must implement _update_strategies\")\n",
    "\n",
    "    def get_action_state(self):\n",
    "        return {\n",
    "            node_id: (1 if agent.strategy.action == \"D\" else 0)\n",
    "            for node_id, agent in self.agents.items()\n",
    "        }\n",
    "\n",
    "    def step(self):\n",
    "        self._play_round()\n",
    "        self._update_strategies()\n",
    "        self.snapshots.append(self.get_action_state())\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.rounds):\n",
    "            self.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b78ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationDynamics(NetworkSimulation):\n",
    "    \"\"\"\n",
    "    Specific implementation of the simulation where agents\n",
    "    adopt the strategy of their most successful neighbor.\n",
    "    \"\"\"\n",
    "\n",
    "    def _update_strategies(self):\n",
    "        \"\"\"Copy the action of the best-performing neighbor (or self).\"\"\"\n",
    "        next_actions = {}\n",
    "\n",
    "        # Iterate over all agents in the generic graph\n",
    "        for node_id in self.graph.nodes:\n",
    "            # Candidates are the node itself + its neighbors\n",
    "            candidates = [node_id] + list(self.network.neighbors(node_id))\n",
    "\n",
    "            # Find the max payoff among candidates\n",
    "            best_score = max(self.agents[candidate].payoff for candidate in candidates)\n",
    "\n",
    "            # Identify all candidates who achieved that score (tie-breaking)\n",
    "            best_nodes = [\n",
    "                candidate\n",
    "                for candidate in candidates\n",
    "                if self.agents[candidate].payoff == best_score\n",
    "            ]\n",
    "\n",
    "            # Randomly choose one of the best performing nodes\n",
    "            chosen = self.random.choice(best_nodes)\n",
    "            next_actions[node_id] = self.agents[chosen].strategy.action\n",
    "\n",
    "        # Apply updates synchronously\n",
    "        for node_id, action in next_actions.items():\n",
    "            self.agents[node_id].strategy.set_action(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83397885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FermiPairwiseComparison(NetworkSimulation):\n",
    "    \"\"\"\n",
    "    Strategy where agents compare payoffs with a single random neighbor\n",
    "    and switch strategies probabilistically based on the Fermi function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph,\n",
    "        rounds=50,\n",
    "        seed=None,\n",
    "        payoff_matrix=payoff_matrices[\"Default\"],\n",
    "        temperature=0.1,\n",
    "        initial_coop_rate=0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            temperature (float): Controls the noise level (K).\n",
    "                                 Lower = more rational (deterministic).\n",
    "                                 Higher = more random.\n",
    "        \"\"\"\n",
    "        super().__init__(graph, rounds, seed, payoff_matrix, initial_coop_rate)\n",
    "        self.K = temperature\n",
    "\n",
    "    def _update_strategies(self):\n",
    "        \"\"\"\n",
    "        Update strategies using the Fermi rule:\n",
    "        P(switch) = 1 / (1 + exp(-(payoff_neighbor - payoff_self) / K))\n",
    "        \"\"\"\n",
    "        next_actions = {}\n",
    "\n",
    "        for node_id in self.graph.nodes:\n",
    "            # 1. Select one random neighbor\n",
    "            neighbors = list(self.network.neighbors(node_id))\n",
    "            if not neighbors:\n",
    "                # Isolated node keeps current strategy\n",
    "                next_actions[node_id] = self.agents[node_id].strategy.action\n",
    "                continue\n",
    "\n",
    "            target_neighbor = self.random.choice(neighbors)\n",
    "\n",
    "            # 2. Compare Payoffs\n",
    "            payoff_self = self.agents[node_id].payoff\n",
    "            payoff_target = self.agents[target_neighbor].payoff\n",
    "            delta = payoff_target - payoff_self\n",
    "\n",
    "            # 3. Calculate Switching Probability (Fermi Function)\n",
    "            # Clip delta/K to avoid overflow in exp() for very low K or high payoffs\n",
    "            # If K is very small, delta/K can be huge.\n",
    "            try:\n",
    "                exponent = -delta / self.K\n",
    "                # Limit exponent to avoid Math Overflow Error\n",
    "                exponent = max(min(exponent, 700), -700)\n",
    "                probability = 1 / (1 + math.exp(exponent))\n",
    "            except ZeroDivisionError:\n",
    "                # If K is 0, we act deterministically (Step function)\n",
    "                probability = 1.0 if delta > 0 else 0.0\n",
    "\n",
    "            # 4. Decide whether to switch\n",
    "            if self.random.random() < probability:\n",
    "                next_actions[node_id] = self.agents[target_neighbor].strategy.action\n",
    "            else:\n",
    "                next_actions[node_id] = self.agents[node_id].strategy.action\n",
    "\n",
    "        # Apply updates synchronously\n",
    "        for node_id, action in next_actions.items():\n",
    "            self.agents[node_id].strategy.set_action(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcementLearning(NetworkSimulation):\n",
    "    \"\"\"\n",
    "    Strategy where agents learn from their own experience using Q-Learning.\n",
    "    Agents maintain Q-values for 'C' and 'D' and update them based on rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph,\n",
    "        rounds=50,\n",
    "        seed=None,\n",
    "        payoff_matrix=payoff_matrices[\"Default\"],\n",
    "        learning_rate=0.1,\n",
    "        epsilon=0.1,\n",
    "        initial_q=0.0,\n",
    "        initial_coop_rate=0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            learning_rate (float): How fast new info overrides old info (alpha).\n",
    "            epsilon (float): Probability of choosing a random action (exploration).\n",
    "            initial_q (float): Starting value for Q-tables (optimistic vs pessimistic).\n",
    "        \"\"\"\n",
    "        super().__init__(graph, rounds, seed, payoff_matrix, initial_coop_rate)\n",
    "        self.alpha = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Initialize Q-tables for every agent: {node_id: {'C': val, 'D': val}}\n",
    "        self.q_tables = {\n",
    "            node_id: {\"C\": initial_q, \"D\": initial_q} for node_id in self.graph.nodes\n",
    "        }\n",
    "\n",
    "    def _update_strategies(self):\n",
    "        \"\"\"\n",
    "        1. Update Q-values based on the reward received in the *previous* round.\n",
    "        2. Select the *next* action using epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Update Q-values (Learning Step)\n",
    "        # We need to know what they *just* played and what they earned.\n",
    "        for node_id, agent in self.agents.items():\n",
    "            action_taken = agent.strategy.action\n",
    "            reward_received = agent.payoff\n",
    "\n",
    "            # Q(A) <- Q(A) + alpha * (Reward - Q(A))\n",
    "            current_q = self.q_tables[node_id][action_taken]\n",
    "            new_q = current_q + self.alpha * (reward_received - current_q)\n",
    "            self.q_tables[node_id][action_taken] = new_q\n",
    "\n",
    "        # 2. Select Next Action (Decision Step)\n",
    "        next_actions = {}\n",
    "        for node_id in self.agents:\n",
    "            # Epsilon-Greedy: Explore with prob epsilon, Exploit otherwise\n",
    "            if self.random.random() < self.epsilon:\n",
    "                next_action = self.random.choice([\"C\", \"D\"])\n",
    "            else:\n",
    "                # Exploit: Choose action with highest Q-value\n",
    "                q_vals = self.q_tables[node_id]\n",
    "                if q_vals[\"C\"] > q_vals[\"D\"]:\n",
    "                    next_action = \"C\"\n",
    "                elif q_vals[\"D\"] > q_vals[\"C\"]:\n",
    "                    next_action = \"D\"\n",
    "                else:\n",
    "                    # Tie-breaking\n",
    "                    next_action = self.random.choice([\"C\", \"D\"])\n",
    "\n",
    "            next_actions[node_id] = next_action\n",
    "\n",
    "        # Apply updates synchronously\n",
    "        for node_id, action in next_actions.items():\n",
    "            self.agents[node_id].strategy.set_action(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91326bdd",
   "metadata": {},
   "source": [
    "## Experiment visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab542719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(\n",
    "    graph,\n",
    "    model_class,\n",
    "    steps=50,\n",
    "    seed=42,\n",
    "    interval=300,\n",
    "    payoff_matrix=None,\n",
    "    is_grid=False,\n",
    "    title=None,\n",
    "):\n",
    "\n",
    "    # Handle default mutable argument if necessary\n",
    "    if payoff_matrix is None:\n",
    "        # Assuming you have a global dict or imported default\n",
    "        payoff_matrix = {}\n",
    "\n",
    "    # --- Configuration ---\n",
    "    C_COOP = \"#40B0A6\"\n",
    "    C_DEFECT = \"#FFBE6A\"\n",
    "\n",
    "    # 1. Initialize Model\n",
    "    model = model_class(graph, rounds=steps, seed=seed, payoff_matrix=payoff_matrix)\n",
    "\n",
    "    # 2. Setup Figure\n",
    "    fig, (ax_sim, ax_stats) = plt.subplots(\n",
    "        2, 1, figsize=(7, 9), gridspec_kw={\"height_ratios\": [4, 1], \"hspace\": 0.3}\n",
    "    )\n",
    "\n",
    "    fig.subplots_adjust(left=0.15, right=0.85, top=0.92, bottom=0.08)\n",
    "    cmap = ListedColormap([C_COOP, C_DEFECT])\n",
    "\n",
    "    # 3. Initialize Stats Tracking & STATIC PLOT ELEMENTS\n",
    "    # We initialize the lines with empty data ONCE.\n",
    "    (line_coop,) = ax_stats.plot(\n",
    "        [],\n",
    "        [],\n",
    "        label=\"Collaborators\",\n",
    "        color=C_COOP,\n",
    "        linewidth=2,\n",
    "        solid_capstyle=\"round\",\n",
    "        path_effects=[\n",
    "            pe.Stroke(linewidth=3, foreground=\"black\", alpha=0.1),\n",
    "            pe.Normal(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    (line_defect,) = ax_stats.plot(\n",
    "        [], [], label=\"Defectors\", color=C_DEFECT, linewidth=2\n",
    "    )\n",
    "\n",
    "    # Set formatting immediately (since we aren't clearing anymore)\n",
    "    ax_stats.set_xlim(0, steps)\n",
    "    ax_stats.set_ylim(0, 100)\n",
    "    ax_stats.yaxis.set_major_formatter(PercentFormatter(xmax=100))\n",
    "    ax_stats.set_ylabel(\"Population\")\n",
    "    ax_stats.grid(True, linestyle=\":\", alpha=0.4)\n",
    "    # ax_stats.legend() # Optional: Add legend if you want it on the bottom graph too\n",
    "\n",
    "    history_coop = []\n",
    "    history_defect = []\n",
    "    steps_range = []\n",
    "    total_nodes = len(graph.nodes)\n",
    "\n",
    "    def update_stats(frame):\n",
    "        state = model.get_action_state()\n",
    "        defector_count = sum(state.values())\n",
    "        cooperator_count = len(state) - defector_count\n",
    "\n",
    "        # Calculate Percentages\n",
    "        pct_defect = (defector_count / total_nodes) * 100\n",
    "        pct_coop = (cooperator_count / total_nodes) * 100\n",
    "\n",
    "        history_defect.append(pct_defect)\n",
    "        history_coop.append(pct_coop)\n",
    "        steps_range.append(frame)\n",
    "\n",
    "        # --- KEY FIX ---\n",
    "        # Update the data of the existing lines instead of clearing/replotting\n",
    "        line_coop.set_data(steps_range, history_coop)\n",
    "        line_defect.set_data(steps_range, history_defect)\n",
    "\n",
    "        # Return the artists that changed\n",
    "        return [line_coop, line_defect]\n",
    "\n",
    "    # 4. Define Visualization Logic\n",
    "    viz_objects = {}\n",
    "\n",
    "    if is_grid:\n",
    "        grid_dim = int(math.isqrt(total_nodes))\n",
    "        if grid_dim * grid_dim != total_nodes:\n",
    "            raise ValueError(f\"Graph has {total_nodes} nodes, not a square.\")\n",
    "\n",
    "        def get_grid_data():\n",
    "            state = model.get_action_state()\n",
    "            grid = [[0 for _ in range(grid_dim)] for _ in range(grid_dim)]\n",
    "            for node_id, is_defector in state.items():\n",
    "                row = node_id // grid_dim\n",
    "                col = node_id % grid_dim\n",
    "                grid[row][col] = is_defector\n",
    "            return grid\n",
    "\n",
    "        viz_objects[\"image\"] = ax_sim.imshow(\n",
    "            get_grid_data(), cmap=cmap, vmin=0, vmax=1, aspect=\"auto\"\n",
    "        )\n",
    "        ax_sim.set_xticks([])\n",
    "        ax_sim.set_yticks([])\n",
    "\n",
    "        def update_viz(frame):\n",
    "            viz_objects[\"image\"].set_data(get_grid_data())\n",
    "            return [viz_objects[\"image\"]]\n",
    "\n",
    "    else:\n",
    "        pos = nx.spring_layout(graph, seed=seed)\n",
    "        nodelist = list(graph.nodes())\n",
    "\n",
    "        nx.draw_networkx_edges(graph, pos, ax=ax_sim, alpha=0.3, edge_color=\"gray\")\n",
    "\n",
    "        state = model.get_action_state()\n",
    "        initial_colors = [state[n] for n in nodelist]\n",
    "\n",
    "        viz_objects[\"nodes\"] = nx.draw_networkx_nodes(\n",
    "            graph,\n",
    "            pos,\n",
    "            nodelist=nodelist,\n",
    "            node_color=initial_colors,\n",
    "            cmap=cmap,\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            node_size=100,\n",
    "            edgecolors=\"gray\",\n",
    "            ax=ax_sim,\n",
    "        )\n",
    "        ax_sim.axis(\"off\")\n",
    "        ax_sim.set_aspect(\"auto\")\n",
    "\n",
    "        def update_viz(frame):\n",
    "            current_state = model.get_action_state()\n",
    "            new_colors = [current_state[n] for n in nodelist]\n",
    "            viz_objects[\"nodes\"].set_array(new_colors)\n",
    "            return [viz_objects[\"nodes\"]]\n",
    "\n",
    "    # 5. Shared Legend & Animation Setup\n",
    "    legend_handles = [\n",
    "        mpatches.Patch(color=C_COOP, label=\"Collaborators\"),\n",
    "        mpatches.Patch(color=C_DEFECT, label=\"Defectors\"),\n",
    "    ]\n",
    "    ax_sim.legend(\n",
    "        handles=legend_handles,\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, -0.02),\n",
    "        ncol=2,\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    ax_sim.set_title(f\"{title if title else ''} (Step 0/{steps})\")\n",
    "\n",
    "    # Initial data load\n",
    "    update_stats(0)\n",
    "\n",
    "    def update(frame):\n",
    "        if frame > 0:\n",
    "            model.step()\n",
    "\n",
    "        ax_sim.set_title(f\"{title if title else ''} (Step {frame}/{steps})\")\n",
    "\n",
    "        # Collect modified artists from both subplots\n",
    "        artists_stats = update_stats(frame)\n",
    "        artists_sim = update_viz(frame)\n",
    "\n",
    "        # Return combined list of artists (crucial for blit=True, good practice for blit=False)\n",
    "        return artists_sim + artists_stats\n",
    "\n",
    "    # Note: blit=True is generally recommended now that we have stable artists\n",
    "    animation = FuncAnimation(\n",
    "        fig, update, frames=steps + 1, interval=interval, blit=False, repeat=False\n",
    "    )\n",
    "\n",
    "    return animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935319ae",
   "metadata": {},
   "source": [
    "## Usage\n",
    "### Strategies and matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4faa034",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = {\n",
    "    \"Imitation Dynamics\": ImitationDynamics,\n",
    "    \"Reinforcement Learning (0.2)\": partial(ReinforcementLearning, learning_rate=0.2),\n",
    "    \"Reinforcement Learning (0.5)\": partial(ReinforcementLearning, learning_rate=0.5),\n",
    "    \"Fermi (0.1)\": partial(FermiPairwiseComparison, temperature=0.1),\n",
    "    \"Fermi (1.0)\": partial(FermiPairwiseComparison, temperature=1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799dcd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "payoff_matrices = {\n",
    "    \"Default\": {\n",
    "        (\"C\", \"C\"): (3, 3),\n",
    "        (\"C\", \"D\"): (0, 5),\n",
    "        (\"D\", \"C\"): (5, 0),\n",
    "        (\"D\", \"D\"): (0, 0),\n",
    "    },\n",
    "    \"Canonical\": {\n",
    "        (\"C\", \"C\"): (-1, -1),\n",
    "        (\"C\", \"D\"): (-3, 0),\n",
    "        (\"D\", \"C\"): (0, -3),\n",
    "        (\"D\", \"D\"): (-2, -2),\n",
    "    },\n",
    "    \"Friend or Foe\": {\n",
    "        (\"C\", \"C\"): (1, 1),\n",
    "        (\"C\", \"D\"): (0, 2),\n",
    "        (\"D\", \"C\"): (2, 0),\n",
    "        (\"D\", \"D\"): (0, 0),\n",
    "    },\n",
    "    \"Snowdrift\": {\n",
    "        (\"C\", \"C\"): (500, 500),\n",
    "        (\"C\", \"D\"): (200, 800),\n",
    "        (\"D\", \"C\"): (800, 200),\n",
    "        (\"D\", \"D\"): (0, 0),\n",
    "    },\n",
    "    \"Prisoners\": {\n",
    "        (\"C\", \"C\"): (500, 500),\n",
    "        (\"C\", \"D\"): (-200, 1200),\n",
    "        (\"D\", \"C\"): (1200, -200),\n",
    "        (\"D\", \"D\"): (0, 0),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0a2370",
   "metadata": {},
   "source": [
    "### Network generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 20\n",
    "grid_graph = generate_graph(\"grid\", size, m=size, periodic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206aacd9",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(strategy_keys, payoff_keys, graphs):\n",
    "    for graph in graphs:\n",
    "        for strategy in strategy_keys:\n",
    "            model = strategies[strategy]\n",
    "            for payoff in payoff_keys:\n",
    "                matrix = payoff_matrices[payoff]\n",
    "                ani = experiment(\n",
    "                    graph=graph,\n",
    "                    model_class=model,\n",
    "                    steps=50,\n",
    "                    seed=42,\n",
    "                    is_grid=True,\n",
    "                    title=f\"{strategy} on a 2D grid using {payoff} matrix\",\n",
    "                    payoff_matrix=matrix,\n",
    "                )\n",
    "                display(ani)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f46f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore(strategies.keys(), payoff_matrices.keys(), graph=grid_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed8828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore(strategies.keys(), [\"Default\"], graphs=[grid_graph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore([\"ImitationDynamics\"], payoff_matrices.keys(), graph=[grid_graph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore(strategies.keys(), [\"Snowdrift\"], graphs=[grid_graph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9cbb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = experiment(\n",
    "    graph=grid_graph,\n",
    "    model_class=strategies[\"Reinforcement Learning (0.2)\"],\n",
    "    steps=50,\n",
    "    seed=42,\n",
    "    is_grid=True,\n",
    "    title=f\"Reinforcement Learning on a 2D grid using Snowdrift matrix\",\n",
    "    payoff_matrix=payoff_matrices[\"Snowdrift\"],\n",
    ")\n",
    "ani.save(\"Test.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for initial_coop in [0.3, 0.5, 0.7]:\n",
    "    model = partial(\n",
    "        ReinforcementLearning, learning_rate=0.2, initial_coop_rate=initial_coop\n",
    "    )\n",
    "    ani = experiment(\n",
    "        graph=grid_graph,\n",
    "        model_class=model,\n",
    "        steps=50,\n",
    "        seed=42,\n",
    "        is_grid=True,\n",
    "        title=f\"Reinforcement Learning on a 2D grid using Snowdrift matrix (initial = {initial_coop})\",\n",
    "        payoff_matrix=payoff_matrices[\"Snowdrift\"],\n",
    "    )\n",
    "    display(ani)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af2d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erdos Renyi\n",
    "# Network generation\n",
    "n_nodes = 400  # equivalent to 20x20 grid\n",
    "p_connection = 0.05  # 5% chance of an edge between any two nodes\n",
    "random_graph = nx.erdos_renyi_graph(n=n_nodes, p=p_connection, seed=42)\n",
    "\n",
    "# Actual experiment\n",
    "ani = experiment(\n",
    "    random_graph,\n",
    "    model_class=partial(\n",
    "        ReinforcementLearning, learning_rate=0.2, initial_coop_rate=0.7\n",
    "    ),\n",
    "    steps=100,\n",
    "    seed=42,\n",
    "    payoff_matrix=payoff_matrices[\"Snowdrift\"],\n",
    "    title=\"Reinforcement Learning using Snowdrift on a Erods Renyi Network\",\n",
    ")\n",
    "display(ani)\n",
    "# ani.save(\"Test-ER.gif\")\n",
    "\n",
    "# Note: cannot do display(ani) and ani.save() on the same instance of an animation, causes issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
