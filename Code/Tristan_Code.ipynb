{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6497bd",
   "metadata": {},
   "source": [
    "# Iterated Prisoner's Dilemma On A Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f664e0b",
   "metadata": {},
   "source": [
    "## Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857538bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "plt.rcParams[\"animation.embed_limit\"] = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7e923",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799dcd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "payoff_matrices = {  # some of these don't look right\n",
    "    \"Default\": {\n",
    "        (\"C\", \"C\"): (3, 3),\n",
    "        (\"C\", \"D\"): (0, 5),\n",
    "        (\"D\", \"C\"): (5, 0),\n",
    "        (\"D\", \"D\"): (0, 0),\n",
    "    },\n",
    "    \"Canonical\": {\n",
    "        (\"C\", \"C\"): (-1, -1),\n",
    "        (\"C\", \"D\"): (-3, 0),\n",
    "        (\"D\", \"C\"): (0, -3),\n",
    "        (\"D\", \"D\"): (-2, -2),\n",
    "    },\n",
    "    \"Friend or Foe\": {\n",
    "        (\"C\", \"C\"): (1, 1),\n",
    "        (\"C\", \"D\"): (0, 2),\n",
    "        (\"D\", \"C\"): (2, 0),\n",
    "        (\"D\", \"D\"): (0, 0),\n",
    "    },\n",
    "    \"Snowdrift\": {\n",
    "        (\"C\", \"C\"): (500, 500),\n",
    "        (\"C\", \"D\"): (200, 800),\n",
    "        (\"D\", \"C\"): (800, 200),\n",
    "        (\"D\", \"D\"): (0, 0),\n",
    "    },\n",
    "    \"Prisoners\": {\n",
    "        (\"C\", \"C\"): (500, 500),\n",
    "        (\"C\", \"D\"): (-200, 1200),\n",
    "        (\"D\", \"C\"): (1200, -200),\n",
    "        (\"D\", \"D\"): (0, 0),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684ac16",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7020407",
   "metadata": {},
   "source": [
    "### Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionStrategy:\n",
    "    \"\"\"Strategy that always plays its current action, randomly initialized.\"\"\"\n",
    "\n",
    "    def __init__(self, rng):\n",
    "        self.rng = rng\n",
    "        self.action = \"C\" if self.rng.random() < 0.5 else \"D\"\n",
    "\n",
    "    def decide(self, agent_history):\n",
    "        return self.action\n",
    "\n",
    "    def set_action(self, action):\n",
    "        self.action = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ead775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationStrategy(ActionStrategy):\n",
    "    \"\"\"Imitate the action with the highest mean payoff in interactions.\"\"\"\n",
    "\n",
    "    def decide(self, agent_history):\n",
    "        totals = {\"C\": 0.0, \"D\": 0.0}\n",
    "        counts = {\"C\": 0, \"D\": 0}\n",
    "        for interactions in agent_history.values():\n",
    "            for inter in interactions:\n",
    "                counts[inter.own_action] += 1\n",
    "                counts[inter.neighbor_action] += 1\n",
    "                totals[inter.own_action] += inter.own_reward\n",
    "                totals[inter.neighbor_action] += inter.neighbor_reward\n",
    "\n",
    "        mean_C = totals[\"C\"] / counts[\"C\"] if counts[\"C\"] else 0\n",
    "        mean_D = totals[\"D\"] / counts[\"D\"] if counts[\"D\"] else 0\n",
    "\n",
    "        if mean_C > mean_D:\n",
    "            self.action = \"C\"\n",
    "        elif mean_D > mean_C:\n",
    "            self.action = \"D\"\n",
    "        else:  # in case of a tie, continue with the existing strategy\n",
    "            pass\n",
    "\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dbd013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FermiStrategy(ActionStrategy):\n",
    "    \"\"\"\n",
    "    Endogenous Fermi algorithm (pairwise).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rng, temperature=0.1):\n",
    "        super().__init__(rng)\n",
    "        self.K = temperature\n",
    "\n",
    "    def decide(self, agent_history):\n",
    "        # if no neighbours yet, keep current action\n",
    "        if not agent_history:\n",
    "            return self.action\n",
    "\n",
    "        # pick one random neighbour we've interacted with\n",
    "        neighbour_id = self.rng.choice(list(agent_history.keys()))\n",
    "        interactions = agent_history.get(neighbour_id, [])\n",
    "        if not interactions:\n",
    "            return self.action\n",
    "\n",
    "        # estimate payoffs from history with that neighbour\n",
    "        own_rewards = [i.own_reward for i in interactions]\n",
    "        neigh_rewards = [i.neighbor_reward for i in interactions]\n",
    "\n",
    "        payoff_self = float(np.mean(own_rewards)) if own_rewards else 0.0\n",
    "        payoff_neigh = float(np.mean(neigh_rewards)) if neigh_rewards else 0.0\n",
    "\n",
    "        delta = payoff_neigh - payoff_self\n",
    "\n",
    "        # Fermi probability\n",
    "        if self.K == 0:\n",
    "            p_switch = 1.0 if delta > 0 else 0.0\n",
    "        else:\n",
    "            exponent = -delta / self.K\n",
    "            exponent = max(min(exponent, 700), -700)  # avoid overflow\n",
    "            p_switch = 1.0 / (1.0 + math.exp(exponent))\n",
    "\n",
    "        # switch action to neighbour's most recently observed action\n",
    "        if self.rng.random() < p_switch:\n",
    "            self.action = interactions[-1].neighbor_action\n",
    "\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcementLearningStrategy(ActionStrategy):\n",
    "    \"\"\"\n",
    "    Q-learning strategy with epsilon-greedy action selection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rng, learning_rate=0.1, epsilon=0.1, initial_q=0.0):\n",
    "        super().__init__(rng)\n",
    "        self.alpha = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.q = {\"C\": float(initial_q), \"D\": float(initial_q)}\n",
    "        self._last_action = None\n",
    "        self._last_reward = 0.0\n",
    "\n",
    "    def decide(self, agent_history):\n",
    "        # observe most recent reward from any interaction (if exists)\n",
    "        last = None\n",
    "        for interactions in agent_history.values():\n",
    "            if interactions:\n",
    "                cand = interactions[-1]\n",
    "                if last is None:\n",
    "                    last = cand\n",
    "        if last is not None:\n",
    "            self._last_reward = last.own_reward\n",
    "\n",
    "        # update Q for the action we previously played\n",
    "        if self._last_action is not None:\n",
    "            a = self._last_action\n",
    "            self.q[a] = self.q[a] + self.alpha * (self._last_reward - self.q[a])\n",
    "\n",
    "        # choose next action (epsilon-greedy)\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            action = \"C\" if self.rng.random() < 0.5 else \"D\"\n",
    "        else:\n",
    "            if self.q[\"C\"] > self.q[\"D\"]:\n",
    "                action = \"C\"\n",
    "            elif self.q[\"D\"] > self.q[\"C\"]:\n",
    "                action = \"D\"\n",
    "            else:\n",
    "                action = \"C\" if self.rng.random() < 0.5 else \"D\"\n",
    "\n",
    "        self.action = action\n",
    "        self._last_action = action\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e4e0a",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Minimal agent holding a strategy, payoff, and history.\"\"\"\n",
    "\n",
    "    @dataclass\n",
    "    class Interaction:\n",
    "        own_action: str\n",
    "        own_reward: float\n",
    "        neighbor_action: str\n",
    "        neighbor_reward: float\n",
    "\n",
    "    def __init__(self, agent_id, strategy, history_window=5, store_history=True):\n",
    "        self.id = agent_id\n",
    "        self.strategy = strategy\n",
    "        self.history = {}\n",
    "        self.payoff = 0.0\n",
    "        self.history_window = history_window\n",
    "        self.store_history = store_history\n",
    "\n",
    "    def choose_action(self):\n",
    "        return self.strategy.decide(self.history)\n",
    "\n",
    "    def record_interaction(\n",
    "        self, neighbor_id, own_action, neighbor_action, own_reward, neighbor_reward\n",
    "    ):\n",
    "        self.payoff += own_reward\n",
    "        if not self.store_history:\n",
    "            return\n",
    "        lst = self.history.setdefault(neighbor_id, [])\n",
    "        lst.append(\n",
    "            self.Interaction(own_action, own_reward, neighbor_action, neighbor_reward)\n",
    "        )\n",
    "        if len(lst) > self.history_window:\n",
    "            del lst[: -self.history_window]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fb92e",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"NetworkX graph wrapper.\"\"\"\n",
    "\n",
    "    def generate_graph(self, kind, n, seed=None, **kwargs):\n",
    "        \"\"\"Generate a networkx graph by name.\"\"\"\n",
    "        if kind == \"grid\":\n",
    "            side_length = int(math.sqrt(n))\n",
    "            graph = nx.convert_node_labels_to_integers(\n",
    "                nx.grid_2d_graph(side_length, side_length)\n",
    "            )\n",
    "\n",
    "        elif kind == \"stochastic_block\":\n",
    "            sizes = kwargs.pop(\"sizes\")\n",
    "            p = kwargs.pop(\"p\")\n",
    "            graph = nx.stochastic_block_model(sizes, p, seed=seed, **kwargs)\n",
    "\n",
    "        else:\n",
    "            generators = {\n",
    "                \"erdos_renyi\": nx.erdos_renyi_graph,\n",
    "                \"watts_strogatz\": nx.watts_strogatz_graph,\n",
    "                \"barabasi_albert\": nx.barabasi_albert_graph,\n",
    "            }\n",
    "            graph = generators[kind](n, seed=seed, **kwargs)\n",
    "\n",
    "        self.kind = kind\n",
    "        self.graph = graph\n",
    "        self.seed = seed\n",
    "\n",
    "    def neighbour(self, agent_id):\n",
    "        \"\"\"Return neighbour agent IDs.\"\"\"\n",
    "        if self.graph:\n",
    "            return list(self.graph.neighbors(agent_id))\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359370c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSimulation(Network):\n",
    "    \"\"\"\n",
    "    Base class for running evolutionary games on any NetworkX graph.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        kind=\"grid\",\n",
    "        n=400,\n",
    "        seed=42,\n",
    "        rounds=100,\n",
    "        strategy=ActionStrategy,\n",
    "        strategy_kwargs=None,\n",
    "        payoff_matrix=payoff_matrices[\"Default\"],\n",
    "        rng=None,\n",
    "        history_window=5,\n",
    "        store_history=True,\n",
    "        store_snapshots=True,\n",
    "        **graph_kwargs,\n",
    "    ):\n",
    "        self.strategy = strategy\n",
    "        self.strategy_kwargs = strategy_kwargs or {}\n",
    "        self.rounds = rounds\n",
    "        self.payoff_matrix = payoff_matrix\n",
    "        self.rng = rng if rng is not None else np.random.default_rng(seed)\n",
    "        self.history_window = history_window\n",
    "        self.store_history = store_history\n",
    "        self.store_snapshots = store_snapshots\n",
    "        self.generate_graph(kind=kind, n=n, seed=seed, **graph_kwargs)\n",
    "        print(nx.degree_histogram(self.graph))\n",
    "        print(nx.average_clustering(self.graph))\n",
    "        self.edge_list = list(self.graph.edges())\n",
    "        self.agents = {}\n",
    "        self.snapshots = []\n",
    "        self._initialize_agents()\n",
    "\n",
    "    def _initialize_agents(self):\n",
    "        for agent_id in self.graph.nodes:\n",
    "            strat = self.strategy(self.rng, **self.strategy_kwargs)\n",
    "            self.agents[agent_id] = Agent(\n",
    "                agent_id,\n",
    "                strat,\n",
    "                history_window=self.history_window,\n",
    "                store_history=self.store_history,\n",
    "            )\n",
    "\n",
    "    def _reset_payoffs(self):\n",
    "        for agent in self.agents.values():\n",
    "            agent.payoff = 0.0\n",
    "\n",
    "    # fast inner loop: precompute lookups and actions once\n",
    "    def _play_round(self):\n",
    "        agents = self.agents\n",
    "        payoff_matrix = self.payoff_matrix\n",
    "        edge_list = self.edge_list\n",
    "\n",
    "        for agent in agents.values():\n",
    "            agent.payoff = 0.0\n",
    "\n",
    "        actions = {node: agents[node].choose_action() for node in agents}\n",
    "        for node_a, node_b in edge_list:\n",
    "            action_a = actions[node_a]\n",
    "            action_b = actions[node_b]\n",
    "            payoff_a, payoff_b = payoff_matrix.get((action_a, action_b), (0, 0))\n",
    "            agents[node_a].record_interaction(\n",
    "                node_b, action_a, action_b, payoff_a, payoff_b\n",
    "            )\n",
    "            agents[node_b].record_interaction(\n",
    "                node_a, action_b, action_a, payoff_b, payoff_a\n",
    "            )\n",
    "\n",
    "    def _get_state(self):\n",
    "        return {\n",
    "            node_id: (1 if agent.strategy.action == \"D\" else 0)\n",
    "            for node_id, agent in self.agents.items()\n",
    "        }\n",
    "\n",
    "    def encode_state(self):  # for speed\n",
    "        arr = np.fromiter(\n",
    "            (\n",
    "                1 if self.agents[i].strategy.action == \"D\" else 0\n",
    "                for i in self.graph.nodes()\n",
    "            ),\n",
    "            dtype=np.uint8,\n",
    "            count=self.graph.number_of_nodes(),\n",
    "        )\n",
    "        return np.packbits(arr, bitorder=\"little\").tobytes()\n",
    "\n",
    "    def decode_state(self, packed):\n",
    "        n = self.graph.number_of_nodes()\n",
    "        bits = np.unpackbits(np.frombuffer(packed, dtype=np.uint8), bitorder=\"little\")\n",
    "        return bits[:n].astype(np.uint8)\n",
    "\n",
    "    def run_until_attractor(\n",
    "        self,\n",
    "        max_steps=2000,\n",
    "        check_every=1,\n",
    "        store_cycle_states=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Detect fixed points or cycles by hashing compact states\n",
    "        \"\"\"\n",
    "        seen = {}\n",
    "        cache = []\n",
    "        for t in range(max_steps + 1):\n",
    "            key = None\n",
    "            if t % check_every == 0:\n",
    "                key = self.encode_state()\n",
    "                if key in seen:\n",
    "                    t0 = seen[key]\n",
    "                    period = t - t0\n",
    "                    attractor = \"fixed\" if period == 1 else \"cycle\"\n",
    "                    cycle_states = cache[t0:t] if store_cycle_states else None\n",
    "                    return {\n",
    "                        \"t_end\": t,\n",
    "                        \"t_cycle_start\": t0,\n",
    "                        \"period\": period,\n",
    "                        \"attractor\": attractor,\n",
    "                        \"cycle_states\": cycle_states,\n",
    "                    }\n",
    "                seen[key] = t\n",
    "\n",
    "            if store_cycle_states:\n",
    "                if key is None:\n",
    "                    key = self.encode_state()\n",
    "                cache.append(key)\n",
    "\n",
    "            self.step()\n",
    "\n",
    "        return {\n",
    "            \"t_end\": max_steps,\n",
    "            \"t_cycle_start\": None,\n",
    "            \"period\": None,\n",
    "            \"attractor\": \"unknown\",\n",
    "            \"cycle_states\": None,\n",
    "        }\n",
    "\n",
    "    def cooperation_metrics(self, state01=None):\n",
    "        \"\"\"\n",
    "        Cluster metrics for cooperators using NetworkX components\n",
    "        \"\"\"\n",
    "        if state01 is None:\n",
    "            state01 = np.fromiter(\n",
    "                (\n",
    "                    1 if self.agents[i].strategy.action == \"D\" else 0\n",
    "                    for i in self.graph.nodes()\n",
    "                ),\n",
    "                dtype=np.uint8,\n",
    "                count=self.graph.number_of_nodes(),\n",
    "            )\n",
    "        coop_nodes = [\n",
    "            node for node, val in zip(self.graph.nodes(), state01) if val == 0\n",
    "        ]\n",
    "        if not coop_nodes:\n",
    "            return {\n",
    "                \"coop_frac\": 0.0,\n",
    "                \"n_coop_clusters\": 0,\n",
    "                \"largest_coop_cluster\": 0,\n",
    "                \"mean_coop_cluster_size\": 0.0,\n",
    "            }\n",
    "        H = self.graph.subgraph(coop_nodes)\n",
    "        comps = list(nx.connected_components(H))\n",
    "        sizes = sorted([len(c) for c in comps], reverse=True)\n",
    "        return {\n",
    "            \"coop_frac\": float((state01 == 0).mean()),\n",
    "            \"n_coop_clusters\": int(len(sizes)),\n",
    "            \"largest_coop_cluster\": int(sizes[0]) if sizes else 0,\n",
    "            \"mean_coop_cluster_size\": float(np.mean(sizes)) if sizes else 0.0,\n",
    "        }\n",
    "\n",
    "    def step(self):\n",
    "        self._play_round()\n",
    "        if self.store_snapshots:\n",
    "            self.snapshots.append(self._get_state())\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.rounds):\n",
    "            self.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28347de",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oscillation_summary(decoded_cycle_states):\n",
    "    \"\"\"\n",
    "    Generate summary of oscillations over time.\n",
    "    \"\"\"\n",
    "    A = np.stack(decoded_cycle_states, axis=0)\n",
    "    oscillates_mask = A.min(axis=0) != A.max(axis=0)\n",
    "    frozen_C = (~oscillates_mask) & (A[0] == 0)\n",
    "    frozen_D = (~oscillates_mask) & (A[0] == 1)\n",
    "    return {\n",
    "        \"frac_oscillating\": float(np.mean(oscillates_mask)),\n",
    "        \"frac_frozen_C\": float(np.mean(frozen_C)),\n",
    "        \"frac_frozen_D\": float(np.mean(frozen_D)),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_many(\n",
    "    kind,\n",
    "    n,\n",
    "    payoff_matrix,\n",
    "    strategy_class,\n",
    "    strategy_kwargs,\n",
    "    seeds,\n",
    "    max_steps=1500,\n",
    "    graph_kwargs=None,\n",
    "    history_window=5,\n",
    "    store_history=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper to run several simulations and compute metrics.\n",
    "    \"\"\"\n",
    "    graph_kwargs = graph_kwargs or {}\n",
    "    strategy_kwargs = strategy_kwargs or {}\n",
    "    rows = []\n",
    "    for seed in seeds:\n",
    "        model = NetworkSimulation(\n",
    "            kind=kind,\n",
    "            n=n,\n",
    "            seed=seed,\n",
    "            rounds=max_steps,\n",
    "            payoff_matrix=payoff_matrix,\n",
    "            strategy=strategy_class,\n",
    "            strategy_kwargs=strategy_kwargs,\n",
    "            store_snapshots=False,\n",
    "            history_window=history_window,\n",
    "            store_history=store_history,\n",
    "            **graph_kwargs,\n",
    "        )\n",
    "        res = model.run_until_attractor(max_steps=max_steps, store_cycle_states=True)\n",
    "        row = {\n",
    "            \"seed\": seed,\n",
    "            \"attractor\": res[\"attractor\"],\n",
    "            \"period\": res[\"period\"],\n",
    "            \"t_end\": res[\"t_end\"],\n",
    "        }\n",
    "\n",
    "        if res[\"cycle_states\"]:\n",
    "            decoded = [model.decode_state(s) for s in res[\"cycle_states\"]]\n",
    "            osc = oscillation_summary(decoded)\n",
    "            row.update(osc)\n",
    "\n",
    "            mets = [model.cooperation_metrics(state01=st) for st in decoded]\n",
    "            row[\"cycle_mean_coop_frac\"] = float(np.mean([m[\"coop_frac\"] for m in mets]))\n",
    "            row[\"cycle_mean_largest_cluster\"] = float(\n",
    "                np.mean([m[\"largest_coop_cluster\"] for m in mets])\n",
    "            )\n",
    "        else:\n",
    "            m = model.cooperation_metrics()\n",
    "            row[\"cycle_mean_coop_frac\"] = m[\"coop_frac\"]\n",
    "            row[\"cycle_mean_largest_cluster\"] = m[\"largest_coop_cluster\"]\n",
    "            row[\"frac_oscillating\"] = np.nan\n",
    "            row[\"frac_frozen_C\"] = np.nan\n",
    "            row[\"frac_frozen_D\"] = np.nan\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def cluster_size_distribution(graph, state01):\n",
    "    \"\"\"\n",
    "    Cacluculate cluster size distribution.\n",
    "    \"\"\"\n",
    "    coop_nodes = [node for node, val in zip(graph.nodes(), state01) if val == 0]\n",
    "    if not coop_nodes:\n",
    "        return {}\n",
    "    H = graph.subgraph(coop_nodes)\n",
    "    comps = list(nx.connected_components(H))\n",
    "    sizes = [len(c) for c in comps]\n",
    "    dist = {}\n",
    "    for size in sizes:\n",
    "        dist[size] = dist.get(size, 0) + 1\n",
    "    return dist\n",
    "\n",
    "\n",
    "def track_cooperation_over_time(\n",
    "    model,\n",
    "    steps=200,\n",
    "    sample_every=1,\n",
    "    max_cluster_size=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample cooperation and cluster sizes over time without storing full snapshots\n",
    "    \"\"\"\n",
    "    metric_rows = []\n",
    "    dist_rows = []\n",
    "\n",
    "    for t in range(steps + 1):\n",
    "        if t % sample_every == 0:\n",
    "            state01 = np.fromiter(\n",
    "                (\n",
    "                    1 if model.agents[i].strategy.action == \"D\" else 0\n",
    "                    for i in model.graph.nodes()\n",
    "                ),\n",
    "                dtype=np.uint8,\n",
    "                count=model.graph.number_of_nodes(),\n",
    "            )\n",
    "            mets = model.cooperation_metrics(state01=state01)\n",
    "            mets[\"t\"] = t\n",
    "            metric_rows.append(mets)\n",
    "\n",
    "            dist = cluster_size_distribution(model.graph, state01)\n",
    "            for size, count in dist.items():\n",
    "                size_key = size\n",
    "                if max_cluster_size is not None and size > max_cluster_size:\n",
    "                    size_key = max_cluster_size + 1\n",
    "                dist_rows.append({\"t\": t, \"size\": size_key, \"count\": count})\n",
    "\n",
    "        model.step()\n",
    "\n",
    "    metrics_df = pd.DataFrame(metric_rows)\n",
    "    dist_df = pd.DataFrame(dist_rows)\n",
    "    return metrics_df, dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91326bdd",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce64db6",
   "metadata": {},
   "source": [
    "### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab542719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(\n",
    "    model_class,\n",
    "    strategy_class,\n",
    "    strategy_kwargs=None,\n",
    "    steps=100,\n",
    "    seed=42,\n",
    "    interval=300,\n",
    "    payoff_matrix=payoff_matrices[\"Default\"],\n",
    "    title=\"\",\n",
    "    kind=\"grid\",\n",
    "    n=400,\n",
    "    is_grid=False,\n",
    "    **graph_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Produce animations showing the network state over time.\n",
    "    \"\"\"\n",
    "    payoff_matrix = payoff_matrix\n",
    "    strategy_kwargs = strategy_kwargs or {}\n",
    "    model = model_class(\n",
    "        kind=kind,\n",
    "        n=n,\n",
    "        seed=seed,\n",
    "        rounds=steps,\n",
    "        payoff_matrix=payoff_matrix,\n",
    "        strategy=strategy_class,\n",
    "        strategy_kwargs=strategy_kwargs,\n",
    "        **graph_kwargs,\n",
    "    )\n",
    "\n",
    "    graph = model.graph\n",
    "    n_nodes = graph.number_of_nodes()\n",
    "\n",
    "    C_COOP, C_DEFECT = \"#40B0A6\", \"#FFBE6A\"\n",
    "    cmap = ListedColormap([C_COOP, C_DEFECT])\n",
    "    fig, (ax_sim, ax_stats) = plt.subplots(\n",
    "        2, 1, figsize=(7, 8), gridspec_kw={\"height_ratios\": [4, 1]}\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Stats plot (C% and D%)\n",
    "    # -------------------------\n",
    "    xs, ys_c, ys_d = [], [], []\n",
    "\n",
    "    (line_c,) = ax_stats.plot([], [], lw=2, label=\"C\")\n",
    "    (line_d,) = ax_stats.plot([], [], lw=2, label=\"D\")\n",
    "\n",
    "    ax_stats.set_xlim(0, steps)\n",
    "    ax_stats.set_ylim(0, 100)\n",
    "    ax_stats.yaxis.set_major_formatter(PercentFormatter(xmax=100))\n",
    "    ax_stats.set_ylabel(\"Population\")\n",
    "    ax_stats.grid(True, linestyle=\":\", alpha=0.4)\n",
    "    ax_stats.legend(frameon=False, ncol=2, loc=\"upper right\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Simulation plot\n",
    "    # -------------------------\n",
    "    if is_grid:\n",
    "        dim = int(math.isqrt(n_nodes))\n",
    "        if dim * dim != n_nodes:\n",
    "            raise ValueError(f\"Grid mode needs square number of nodes, got {n_nodes}.\")\n",
    "\n",
    "        def state_as_grid():\n",
    "            state = model._get_state()\n",
    "            grid = [[0] * dim for _ in range(dim)]\n",
    "            for node, val in state.items():\n",
    "                grid[node // dim][node % dim] = val\n",
    "            return grid\n",
    "\n",
    "        sim_artist = ax_sim.imshow(state_as_grid(), cmap=cmap, vmin=0, vmax=1)\n",
    "        ax_sim.set_xticks([])\n",
    "        ax_sim.set_yticks([])\n",
    "\n",
    "        def update_sim():\n",
    "            sim_artist.set_data(state_as_grid())\n",
    "\n",
    "    else:\n",
    "        pos = nx.spring_layout(graph, seed=seed)\n",
    "        nodelist = list(graph.nodes())\n",
    "        nx.draw_networkx_edges(graph, pos, ax=ax_sim, alpha=0.3, edge_color=\"gray\")\n",
    "        state0 = model._get_state()\n",
    "        sim_artist = nx.draw_networkx_nodes(\n",
    "            graph,\n",
    "            pos,\n",
    "            nodelist=nodelist,\n",
    "            node_color=[state0[i] for i in nodelist],\n",
    "            cmap=cmap,\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            node_size=80,\n",
    "            edgecolors=\"gray\",\n",
    "            ax=ax_sim,\n",
    "        )\n",
    "        ax_sim.axis(\"off\")\n",
    "\n",
    "        def update_sim():\n",
    "            state = model._get_state()\n",
    "            sim_artist.set_array([state[i] for i in nodelist])\n",
    "\n",
    "    # -------------------------\n",
    "    # Animation update\n",
    "    # -------------------------\n",
    "    def update(frame):\n",
    "        if frame > 0:\n",
    "            model.step()\n",
    "\n",
    "        ax_sim.set_title(f\"{title} (Step {frame}/{steps})\")\n",
    "\n",
    "        update_sim()\n",
    "\n",
    "        state = model._get_state()\n",
    "        d = sum(state.values())\n",
    "        c = n_nodes - d\n",
    "\n",
    "        xs.append(frame)\n",
    "        ys_c.append(100 * c / n_nodes)\n",
    "        ys_d.append(100 * d / n_nodes)\n",
    "\n",
    "        line_c.set_data(xs, ys_c)\n",
    "        line_d.set_data(xs, ys_d)\n",
    "\n",
    "        return sim_artist, line_c, line_d\n",
    "\n",
    "    anim = FuncAnimation(\n",
    "        fig,\n",
    "        update,\n",
    "        frames=steps + 1,\n",
    "        interval=interval,\n",
    "        blit=True,\n",
    "        repeat=False,\n",
    "    )\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb468d",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_names = [\"Snowdrift\"]\n",
    "size = 10\n",
    "n = size * size\n",
    "\n",
    "runs = [\n",
    "    # (\"Imitation\", ImitationStrategy, {}),\n",
    "    (\n",
    "        \"Reinforcement Learning\",\n",
    "        ReinforcementLearningStrategy,\n",
    "        {\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"epsilon\": 0.1,\n",
    "            \"initial_q\": 0.0,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "graph_setups = [\n",
    "    # (\"Grid\", \"grid\", {}, True),\n",
    "    (\"Small-World\", \"watts_strogatz\", {\"k\": 4, \"p\": 0.1}, False),\n",
    "    (\"Erdos-Renyi\", \"erdos_renyi\", {\"p\": 0.1}, False),\n",
    "]\n",
    "\n",
    "for matrix_name in matrix_names:\n",
    "    matrix = payoff_matrices[matrix_name]\n",
    "\n",
    "    for graph_label, kind, graph_kwargs, is_grid in graph_setups:\n",
    "        for strat_label, strat_cls, strat_kwargs in runs:\n",
    "            ani = experiment(\n",
    "                model_class=NetworkSimulation,\n",
    "                strategy_class=strat_cls,\n",
    "                strategy_kwargs=strat_kwargs,\n",
    "                steps=60,\n",
    "                seed=42,\n",
    "                interval=50,\n",
    "                payoff_matrix=matrix,\n",
    "                kind=kind,\n",
    "                n=n,\n",
    "                is_grid=is_grid,\n",
    "                title=f\"{strat_label} on {matrix_name} ({graph_label})\",\n",
    "                **graph_kwargs,\n",
    "            )\n",
    "            display(ani)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95747c37",
   "metadata": {},
   "source": [
    "## WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ddc612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_many(\n",
    "    kind=\"watts_strogatz\",\n",
    "    n=400,\n",
    "    payoff_matrix=payoff_matrices[\"Snowdrift\"],\n",
    "    strategy_class=ImitationStrategy,\n",
    "    strategy_kwargs={},\n",
    "    seeds=range(30),\n",
    "    max_steps=1500,\n",
    "    graph_kwargs={\"k\": 4, \"p\": 0.1},\n",
    ")\n",
    "print(df.head())\n",
    "print(\n",
    "    df.groupby(\"attractor\")[\n",
    "        [\n",
    "            \"period\",\n",
    "            \"cycle_mean_coop_frac\",\n",
    "            \"cycle_mean_largest_cluster\",\n",
    "            \"frac_oscillating\",\n",
    "        ]\n",
    "    ].agg([\"mean\", \"std\", \"median\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cooperation over time + cluster size distribution\n",
    "model = NetworkSimulation(\n",
    "    kind=\"watts_strogatz\",\n",
    "    n=400,\n",
    "    seed=1,\n",
    "    rounds=300,\n",
    "    payoff_matrix=payoff_matrices[\"Snowdrift\"],\n",
    "    strategy=ImitationStrategy,\n",
    "    strategy_kwargs={},\n",
    "    store_snapshots=False,\n",
    "    history_window=5,\n",
    "    store_history=True,\n",
    "    k=4,\n",
    "    p=0.1,\n",
    ")\n",
    "\n",
    "time_metrics, size_dist = track_cooperation_over_time(\n",
    "    model, steps=300, sample_every=2, max_cluster_size=40\n",
    ")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 7), sharex=True)\n",
    "ax1.plot(time_metrics[\"t\"], time_metrics[\"coop_frac\"], label=\"Coop fraction\")\n",
    "ax1.plot(\n",
    "    time_metrics[\"t\"],\n",
    "    time_metrics[\"largest_coop_cluster\"],\n",
    "    label=\"Largest coop cluster\",\n",
    ")\n",
    "ax1.set_ylabel(\"Value\")\n",
    "ax1.grid(True, linestyle=\":\", alpha=0.4)\n",
    "ax1.legend(frameon=False)\n",
    "\n",
    "if not size_dist.empty:\n",
    "    pivot = size_dist.pivot_table(\n",
    "        index=\"t\", columns=\"size\", values=\"count\", aggfunc=\"sum\", fill_value=0\n",
    "    )\n",
    "    ax2.stackplot(pivot.index, pivot.values.T, labels=pivot.columns)\n",
    "    ax2.set_ylabel(\"Cluster count\")\n",
    "    ax2.set_xlabel(\"t\")\n",
    "    ax2.grid(True, linestyle=\":\", alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Batch summary plots over seeds\n",
    "batch_df = run_many(\n",
    "    kind=\"watts_strogatz\",\n",
    "    n=400,\n",
    "    payoff_matrix=payoff_matrices[\"Snowdrift\"],\n",
    "    strategy_class=ImitationStrategy,\n",
    "    strategy_kwargs={},\n",
    "    seeds=range(30),\n",
    "    max_steps=1500,\n",
    "    graph_kwargs={\"k\": 4, \"p\": 0.1},\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "axes[0].hist(batch_df[\"period\"].dropna(), bins=20)\n",
    "axes[0].set_title(\"Period\")\n",
    "axes[1].hist(batch_df[\"cycle_mean_coop_frac\"].dropna(), bins=20)\n",
    "axes[1].set_title(\"Cycle mean coop\")\n",
    "axes[2].hist(batch_df[\"cycle_mean_largest_cluster\"].dropna(), bins=20)\n",
    "axes[2].set_title(\"Cycle mean largest cluster\")\n",
    "for ax in axes:\n",
    "    ax.grid(True, linestyle=\":\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
